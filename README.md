# Awesome LLM Resources

## Part 1
1. BloombergGPT: A Large Language Model for Finance (2023) by Wu et al., [Link](https://arxiv.org/abs/2303.17564)
2. Towards Expert-Level Medical Question Answering with Large
Language Models (2023) by Singhal et al., [Link](https://arxiv.org/abs/2305.09617)
3. Attention Is All You Need (2017) by Vaswani et al., [Link](https://arxiv.org/abs/1706.03762)
4. BERT: Pre-training of Deep Bidirectional Transformers for Language
Understanding (2018) by Devlin et al., [Link](https://arxiv.org/abs/1810.04805)
5. Language Models are Few-Shot Learners (2020) by Brown et al., [Link](https://arxiv.org/abs/2005.14165)
6. An Image is Worth 16x16 Words: Transformers for Image Recognition
at Scale (2020) by Dosovitskiy et al., [Link](https://arxiv.org/abs/2010.11929)
7. RWKV: Reinventing RNNs for the Transformer Era (2023) by Peng et
al., [Link](https://arxiv.org/abs/2305.13048)
8. Hyena Hierarchy: Towards Larger Convolutional Language Models
(2023) by Poli et al., [Link](https://arxiv.org/abs/2302.10866)
9. Mamba: Linear-Time Sequence Modeling with Selective State Spaces
(2023) by Gu and Dao, [Link](https://arxiv.org/abs/2312.00752)
10. Llama 2: Open Foundation and Fine-Tuned Chat Models (2023) by
Touvron et al., [Link](https://arxiv.org/abs/2307.092881)
11. The Pile: An 800GB Dataset of Diverse Text for Language Modeling
(2020) by Gao et al., [Link](https://arxiv.org/abs/2101.00027)
12. Training Language Models to Follow Instructions with Human
Feedback (2022) by Ouyang et al., [Link](https://arxiv.org/abs/2203.02155)


## Part 2
13. Machine Learning Q and AI (2023) by Sebastian Raschka, [Link](https://leanpub.com/machine-learning-q-and-ai)
14. Neural Machine Translation of Rare Words with Subword Units (2015)
by Sennrich at al., [Link](https://arxiv.org/abs/1508.07909)
15. Code for the Byte pair encoding tokenizer used to train GPT-2 open-sourced by OpenAI, [Link](https://github.com/openai/gpt-2/blob/master/src/encoder.py)

16. Interactive web UI to illustrate how the byte pair tokenizer in GPT model works, [Link](https://platform.openai.com/tokenizer)
17. A minimal implementation of a BPE tokenizer by Andrej Karpathy, [Link](https://github.com/karpathy/minbpe)
18. SentencePiece: A Simple and Language Independent Subword
Tokenizer and Detokenizer for Neural Text Processing (2018) by Kudo
and Richardson, [Link](https://aclanthology.org/D18-2012/)
19. Fast WordPiece Tokenization (2020) by Song et al., [Link](https://arxiv.org/abs/2012.15524)


## Part 3
